%-----------------------------------------------------------------------------
%
%               Template for sigplanconf LaTeX Class
%
% Name:         sigplanconf-template.tex
%
% Purpose:      A template for sigplanconf.cls, which is a LaTeX 2e class
%               file for SIGPLAN conference proceedings.
%
% Author:       Paul C. Anagnostopoulos
%               Windfall Software
%               978 371-2316
%               paul@windfall.com
%
% Created:      15 February 2005
%
%-----------------------------------------------------------------------------


\documentclass[preprint]{sigplanconf}

% The following \documentclass options may be useful:
%
% 10pt          To set in 10-point type instead of 9-point.
% 11pt          To set in 11-point type instead of 9-point.
% authoryear    To obtain author/year citation style instead of numeric.

\usepackage{amsmath}
\usepackage[pdftex]{graphicx}
\usepackage[noend]{algorithmic}
\usepackage{algorithm}


\begin{document}

\conferenceinfo{WXYZ '05}{date, City.}
\copyrightyear{2005}
\copyrightdata{[to be supplied]}

\titlebanner{banner above paper title}        % These are ignored unless
\preprintfooter{short description of paper}   % 'preprint' option specified.

\title{Evaluating Graph Coloring on GPUs}

\authorinfo{Name1}
           {Affiliation1}
           {Email1}
\authorinfo{Name2\and Name3}
           {Affiliation2/3}
           {Email2/3}

\maketitle

\begin{abstract}
This paper evaluates features of graph coloring algorithms implemented on graphics processing units (GPUs), comparing coloring heuristics, partitioning strategies, and thread decompositions.  As compared to prior work on parallel graph coloring for other parallel architectures, we find that the large number of cores and relatively high global memory bandwidth of a GPU lead to different strategies for the parallel implementation.  Specifically, we find that a simple block partitioning of nodes often outperforms, in terms of execution time or number of colors, the partition found by METIS, which tries to minimize cross-processor edges.  We also discover that our coloring heuristics lead to the same or fewer colors than a prior approach for a distributed-memory cluster architecture.  Our algorithm resolves many coloring conflicts across partitioned blocks on the GPU by iterating through the coloring process up to three times, before returning to the CPU to resolve remaining conflicts, and this iteration dramatically impacts speedups as compared to a single pass.  Overall, parallel graph coloring speeds up well for the sparse graphs we studied, commonly yielding speedups above 1000X over a baseline CPU implementation.
\end{abstract}

\category{CR-number}{subcategory}{third-level}

\terms
term1, term2, term3

\keywords
Graph coloring, Parallel algorithm, CUDA

\section{Introduction}

edges) subject to certain constraints. In this paper, we consider the specific problem of assigning colors to vertices so that no two neighboring vertices (vertices connected by an edge) have the same color.  There are several known applications of graph coloring like assigning frequencies to wireless access points, time-tabling and scheduling, register allocation and printed circuit testing, iterative solution of sparse linear systems [], preconditions [], sparse tiling [], and eigenvalue computation [].  

Theoretically,  graph coloring has been proven to be NP-complete, and even approximating the chromatic number of a graph is known to be NP-hard [reference?]. Therefore, a number of approximate graph coloring algorithms have been developed that use heuristics to guide assignment of colors; some commonly used heuristics include First Fit, Largest Degree Order and Saturation Degree Order.  These heuristics tend to trade off minimizing the number of colors and minimizing execution time.  Combinations of these algorithms have also been used to create better heuristics. These algorithms are generally based on the same general greedy framework: a. vertex is selected according to some predefined criteria and colored with the smallest valid color. The selection and coloring continues until all the vertices in the graph are colored.

For coloring large graphs, a parallel implementation seems natural as we can split the graph into small subgraphs, color each of them separately in parallel, and then combine results for all of the subgraphs to derive the final coloring.   Occasionally, this combining step will encounter conflicts, where two adjacent vertices that span multiple subgraphs have been assigned the same color.  A parallel algorithm must resolve conflicts, by assigning one of the vertices involved in the conflict a different color.    Processing conflicts partially serializes the computation and may also lead to the use of a higher number of colors.

This paper examines a graphics processing unit (GPU) mapping of parallel graph coloring.  Prior parallel graph coloring algorithms have been evaluated on conventional shared-memory multiprocessors [reference] or distributed systems [reference], but to the best of our knowledge, this is the first study of how GPU architectures affect performance gains and number of assigned colors in a parallel implementation.  Our study demonstrates that features of the GPU architecture significantly impact the algorithms selected.  Specifically, the support for efficient fine-grain multithreading facilitates strong performance gains over CPU implementations (as high as 1000X) because hundreds or even thousands of threads can be applied to the parallel coloring.  Further, multi-threading can hide the latency to memory of the irregular accesses that on a conventional CPU do not effectively utilize caches.  In addition, most parallel graph algorithms seek to partition graphs carefully to minimize the number of edges that cross processor boundaries, since cross-processor accesses typically have very high communication latencies [need a reference, possibly just a METIS reference].  On a GPU, where the entire graph is stored in a global device memory, the latency of accessing vertices inside or outside the current subgraph is no different, making the partitioning process less important. 

Our study also identifies several features of graphs and the GPU implementation that impact both performance and number of assigned colors.  The size and sparsity of a graph affects the number of conflicts, which as suggested above, affects both performance and the quality of the coloring.  Our study evaluates parallelizing polynomial graph coloring algorithms that are known to derive fewer colors than lower complexity algorithms, but for small graphs these more efficient algorithms may also lead to the same quality solution at lower cost.  Therefore, certain graph properties determine whether using a GPU is profitable.  We will evaluate alternative coloring heuristics, partitioning strategies and granularity of the partitioning, to find the solution that leads to a high-quality coloring and a significant performance gain over a baseline CPU implementation.  We compare our coloring results to a prior parallel algorithm on the same set of graphs [from where] [reference?].

The remainder of this paper is structured as follows: we first review previous graph coloring work will be presented.  The following section discusses the details of the parallel algorithm and the coloring heuristics we evaluated.  We present extensive experimental results followed by a conclusion.

\section{Target platform}
GPUs have different architectures compared to parallel computers where most of the parallel graph-coloring algorithms are run. The main differences and how they impact on algorithm are described below:
i) Number of processors. Graphics cards have many processors: an NVIDIA GTX 260 has 192 cores while a Tesla S1070 processor has 240 cores. This allows us to have much more parallelism (and having many more threads running at the same time) compared to other architectures. Moreover, as thread creation is inexpensive on a GPU, we will be running a lot to fully use the power of the GPU. Consequently our algorithm will be using many more threads than existing algorithms.

ii) Processor speed. While a state-of-the-art processor typically operates at around 3 GHz processor cores on GPUs are much slower: one core on a GTX 260 operates at 1242 MHz while a core on a Tesla S1070 operates between 1.296 to 1.44  GHz. Hence we need to use the many threads to match or better the speed of a standard CPU. Moreover, trying to get speedups for  O(N) algorithms are hard as the cores are individually slower and some time is wasted copying data to and from the GPU

iii) Memory. The memory available to parallel computers is normally bigger but it is sharing data between processors is usually much more expensive. On a GPU, all the data can be stored in the global memory and all the cores can readily access it.
\section{ Related Work}
In this section we give an overview of existing graph coloring algorithms, both sequential and parallel.  In this discussion, a graph $G=(V,E)$, where $V$ is a set of vertices, and $E$ is a set of edges.We denote the number of vertices $V$ by $|V|$, the number of edges $E$ by $|E|$. And the degree of a vertex $v_i \in V$ is denoted by $d(v_i)$. The maximum vertex degree of a graph is denoted as $\Delta$.

\subsection{Sequential greedy coloring}

The problem of sequential graph coloring has been studied extensively. Among all the sequential approaches, greedy algorithms are proven to be very effective in practice, such as First Fit, Largest degree ordering, Saturation degree ordering and Incidence degree ordering[citations?]. Such greedy algorithms iterate over the vertices to be colored in a certain order, and at each step assign the current vertex the smallest permissible color. Given a graph $G$, and the maximum degree of vertex in $G$ is $\Delta$, then the vertex to be colored can have at most $\Delta$ neighbors. As a result, the number of colors for greedy algorithms is bounded by the degree of the graph (actually $\Delta+1$) \cite{allwright1995comparison}.  We now describe each of these greedy coloring algorithms in more detail.

1. First Fit (FF): It is the simplest algorithm of all greedy coloring heuristics and the computation complexity is $O(n)$ \cite{klotz2002graph}.To color a vertex, a permissible color is chosen from the interval $[1,C]$, where C is the largest color currently used. Use a new color C+1 if all the C colors have been assigned to its neighboors.

2. Degree based algorithms:
a. Largest degree ordering(LDO):  It chooses a  vertex with largest degree.This algorithm has a computational complexity around $O(n^2)$ \cite{klotz2002graph}

b. Saturation degree ordering (SDO): The saturation degree of a vertex is defined  as the number of different colors its neighbor vertices have been assigned. It can give less colors than LDO and the computational complexity is $O(n^3)$\cite{klotz2002graph}.

c. Incidence degree ordering (IDO): The incidence degree of a vertex is defined as the number of its adjacent colored vertices. It can achieve a computation complexity of $O(n^2)$\cite{klotz2002graph}.

Generally, the First Fit algorithm uses more colors than the degree-based algorithms, but it is the most efficient one. The degree-based algorithms all have a polynomial algorithmic complexity, since all vertices are traversed to determine the next node to color.  In \cite{gebremedhin1999parallel}, it was shown that SDO gives best quality of coloring in terms of  the number of colors among all degree-based greedy algorithms.   However, when more than one vertex has the same degree, it is hard to decide which one to choose for coloring.  To overcome this disadvantage,  some variants are proposed by  \cite{al2006new}, such as the algorithm that combines SDO $\& $LDO.

\renewcommand{\algorithmicrequire}{\textbf{phase}}
Assume the total number of vertices in the graph to be colored is N,
and the ith vertice is $v_i$.
\begin{algorithm}
\begin{algorithmic}
\STATE {\mbox {SDO $\&$ LDO algorithm:}}
\WHILE {$N_{colored} < N $}
    \STATE  $ max = -1$
    \FOR {$ i = 1\: to\: N$}
       \IF {$!colored(v_i)$}
           \STATE $sd = SaturatedDegree(v_i) $
           \IF {$sd > max$}
               \STATE $max = sd$
               \STATE $index = i$
           \ENDIF
           \IF {$ sd = max$}
             \IF {$d(v_i) > d(v_{index})$}
                 \STATE $index = i$
             \ENDIF
           \ENDIF
        \STATE $ Color \:v_{index} $
        \STATE $N_{colored} ++$
        \ENDIF
    \ENDFOR
\ENDWHILE
\end{algorithmic}
\end{algorithm}


\subsection{Previous work on parallel graph coloring}

A number of existing parallel algorithms are based on the concept of maximal independent set (MIS) proposed by \citet{luby1985simple}. An independent set is defined as a set of vertices in which no two vertices are neighbors. For Luby’s MIS algorithm, all the vertices in an independent set are assigned to the same color, and once an independent set is found and colored, the vertices in the set and their neighbors are removed from the graph. By iterating the procedure until all the vertices in the graph are colored, the independent sets found are merged into a maximal independent set. Luby proposed a parallel algorithm of searching for an independent set based on a Monto Carlo method in \cite{luby1985simple}.

Jones-Plassmann’s algorithm \cite{jones1993parallel}  improves the work of Luby. A single set of random weights is constructed and assigned to each vertex before coloring, and it searches for independent sets in parallel just like the MIS algorithm does, selecting the vertices with the (local) largest weights. Instead of assigning the same color to the vertices in an independent set, they are colored by the smallest color that has not been assigned to any neighbors. Gjertsen et al \cite{gjertsen1996parallel} and Allwright et al \cite{allwright1995comparison} share the same idea.  The parallel implementations that follow this approach are targeting distributed memory architectures.  Graphs are uniformly partitioned into subgraphs and each subgraph is associated with a specific processor.  To avoid color conflicts, two adjacent vertices on different processors cannot be colored in the same time step. This restriction can be overcome by Johansson’s algorithm, where each processor is assigned one single vertex and the vertices are colored randomly in the same time step. Color inconsistency may occur in this  scheme, so the coloring procedure is recursively re-run for the vertices receiving illegal colors.

Gebremedhin and Manne proposed a  parallel coloring algorithm for shared memory architectures, which achieves a linear speedup on the PRAM model \cite{gebremedhin2000scalable} .And in \cite{gebremedhin2006speeding} they inproved their work and proposed a G-M algorithm which provides better performance. The G-M algorithm consists of four phases: (1) Partitioning: Assuming the number of processors is p, in this phase the graph is broken down into p independent subgraphs of almost the same size, and each subgraph is assigned to one processor ; (2) Pseudo-coloring: Each processor tentatively colors the vertices assigned to it in sequence. When two adjacent vertices reside on different processors are colored simultaneously, coloring inconsistency may arise.; (3) Conflict detection:  Each processor checks the colors of  vertices assigned to it for consistency and identify a set of vertices which have color conflicts; (4) Conflict resolving: Re-coloring the detected vertices with conflicts.

In this paper, we propose a framework based on the G-M algorithm, which is adapted for an NVIDIA GPU platform.  In addition, two new heuristics are proposed to match the parallelism exploited by the GPU, which are shown to give better results than existing heuristics such as FF and SDO. The proposed framework and algorithms are described in the next section.

\section{Algorithm}

After studying previous work on parallel graph-coloring algorithms, we have developed a CUDA version of the algorithm which is based on the former but is more suited to GPU architectures.

\subsection{The Graph Coloring Framework}

\begin{algorithm}
\begin{algorithmic}
\STATE {\mbox{CUDA-Coloring Framework}}
\linebreak 
\REQUIRE {$1 \: : \: Boundary Nodes - CPU$}
    \STATE{ \mbox{Logically partition graph into subgraphs}}
	\STATE{ \mbox{Indentify boundary nodes}}
	\STATE{ \mbox{Count neighbors outside the subgraph for each node}}
	
\REQUIRE {$2 \: : \: Graph Coloring - GPU$}
	\WHILE {Number of color conflicts is high}
		\STATE{ \mbox{Color graph using the specified heuristic}}
		\STATE{ \mbox{Identify color conflicts}}
	\ENDWHILE

\REQUIRE {$3 \: : \: Sequential Conflicts resolution - CPU$}
	\STATE{ \mbox{Residual conflicts are eliminated}}
		
\end{algorithmic}
\end{algorithm}  


\textbf{Graph Partitioning}\
The graph is initially padded with empty nodes so that it is exactly divisible by the total number of threads. The graph can be partitioned in two ways: either arbitrary regular subgraphs or using a graph partitioning system like METIS[Karypis] (in which case the graph is not padded) and the number of partitions is set to be equal to the number of threads used.
Ideally we would like all the subgraphs to be of the same size. But when METIS is used, the resulting subgraphs tend to be unevenly sized. This is an issue when using GPUs as we would like all the cores to be working roughly the same amount.\

\textbf{Boundary Nodes}\
For many graphs, especially if METIS is not used, the number of boundary nodes is roughly equal to the number of of vertices as we have very small partitions (less than 50 in most cases). However, when METIS is used, checking only boundary nodes can help make Steps 3 $\&$ 4 faster for some graphs.
Moreover, we have designed two heuristics (see section 3.2.3) which base their color allocation on the number of neighbours of a node which are outside the subgraph.\

\textbf{Graph Coloring  and Conflicts detection}\
Each thread assigns colors to its subgraph but checks the whole graph before allocating colors. We tried four strategies for color allocation:
i) First Fit. Works according to the First Fit heuristic. It’s an O(N) algorithm which is fast but whic generally produces many colors

ii) (SDO $\&$ LDO) SDO and LDO Combined. This is an $O(N^3)$ algorithm referred to as Proposed Algorithm 2 from [Sabri]

iii) MAX OUT. This is a  proposed new heuristic that is based on the second strategy. Nodes that are colored first are those having a maximum number of neighbours outside the subgraph and then it based on the largest degree

iv) MIN OUT. A proposed new heuristic which is like the third one but here the nodes which have the minimum number of neighbors outside the subgraph (and have largest degree) are colored first.

Conflicts are detected by checking whether the boundary nodes have any conflicts. If they do, their color is reset to NULL.\

\textbf{Multiple Passes on GPU}\
Sometimes after the first pass of Graph Coloring and Conflicts detection on GPU there are many nodes with conflicts. Solving the conflicts on CPU as is done in many other approaches, like in [Gebremedhin], results in a drastic slowdown of the system. Instead, a second, third, ... as many passes as required are done on the GPU to bring down the total number of conflicts to a minimum before finally eliminating residual conflicts on the sequentially on the CPU.

This step provides drastic increases speedups and most of the time does not increase the number of colors used a lot.

\section{Experiments and Results}
The algorithms have been implemented using the CUDA API and the tests are mainly carried out on a  one processor of a Tesla S1070. The configuration is as follows:
Number of Cores: 240
Memory: ...
CPU: ...

Table 1

\begin{table}
\scriptsize
\begin{center}
\begin{tabular}{l r r r r}
\hline
Name & $|V|$ & $|E|$ & Max Degree & Avg Degree \\
\\
\hline
ct20stif & 52,329 & 1,375,396 & 206 & 50 \\
nasasrb & 54,870 & 1,366,097 & 275 & 47 \\
pwtk  & 217,918 & 5,926,171 & 179 & 52 \\
\\
pkustk10 & 81,920 & 2,114,154 & 89 & 52 \\
pkustk11 & 87,804 & 2,565,054 & 131 & 55 \\
pkustk13 & 94,893 & 3,260,967 & 299 & 68\\
\\
hood & 220,542 & 5,273,947 & 76  & 47 \\
msdoor & 417,792 & 9,912,536 & 76 & 46 \\
ldoor & 958,464 & 22,785,136 & 76 & 46 \\

\end{tabular}
\caption{Information of tested graphs}
\label{amean_time}
\end{center}
\end{table}


\begin{table}
\scriptsize
\begin{center}
\begin{tabular}{l r r r r}
\hline
Name & Colors  & Time  & Colors & Time \\
 & FF & FF &   SDO$\&$LDO &  SDO$\&$LDO\\
\\
\hline
ct20stif & 49 & 29 & 48 & 203387 \\
nasasrb & 41 & 29 & 36 & 646113 \\
pwtk  & 48 & 116 & 48 & 5000000 \\
\\
pkustk10 & 42 & 39 & 42 & 226838 \\
pkustk11 & 57 & 48 & 51 & 250112 \\
pkustk13 & 57 & 66 & 51 & 612960\\
\\
hood & 42 & 105 & 36  & 1709180 \\
msdoor & 42 & 197 & 35 & 7281740 \\
ldoor & 42 & 450 & 35 & 34584800 \\

\end{tabular}
\caption{Sequential coloring results for FF and SDO$\&$LDO}
\label{amean_time}
\end{center}
\end{table}





The table below shows 9 real graphs from [uof]. The graphs have been divided into 3 groups according to their domain:

   1. Structural Engineering

          o ct20stif : CT20 Engine Block stiffness matrix - Boeing
          o nasasrb: Struncture from Nasa Langley , shuttle rocket booster - NASA
          o pwtk: pressurized wind tunnel stifness matrix - Boeing


   

o

   2. Automotive
          * hood:
          * msdoor:
          * ldoor:
    o
         3. Civil Engineering
                * pkustk10:
                * pkustk11:
                * pkustk13:

               

            Their detailed characteristics are shown in table 2 below.  In the last two columns, we also show timing of the two heuristics on the CPU, serving as the baseline.  To obtain these timings, the program is run 10 times and the colors and timings are the average of the 10 runs.

            Say more about the graphs.  Range of similar average degree.  Variation in max-degree and size.  Similar number of colors, but size impacts execution time significantly due to polynomial algorithm.  etc. etc.

(tables and figures are coming soon)

The performance and number of colors resulting from the First Fit heuristic and the Combined SDO$\&$LDO is shown for each graph. As we can see, though First Fit heuristic is much faster than the Combined SDO$\&$LDO algorithm, the resulting number of colors is often much higher.  If number of colors is important to the application, then we would like to achieve the results of the more precise algorithm, but not at the dramatically increased execution time.  (need to quantify how much slower and how much worse is the coloring.)

\subsection{Algorithms Compared}

Now we investigate the impact of parallelizing the combined SDO$\&$LDO for the GPU on both performance and number of colors obtained.  For each graph, we used the following methodology. We varied the number of threads and compared the number of colors and speedup over the CPU baseline for each. We use the sequential combined SDO$\&$LDO CPU code to compare the colors and speedup for the parallel SDO$\&$LDO, MAX and MIN heuristics, while we use the CPU FirstFit as a baseline for the parallel First Fit.

\textbf{Number of threads}\
Given that the graphs have very different sizes, it does not make sense to use the same number of threads for each. For example, for a graph of 50 000 nodes, if 1024 threads are used, each thread handles about 48 vertices while for a graph of 500 000 nodes, each thread will operate on around 480 vertices. Since GPU cores are much slower than state-of-the-art processors speedup gains are the result of using as many cores as possible where each is operating on just a few vertices.

To determine subgraph sizes that tend to give fewer colors, we linearly decreased the size of the subgraph by varying the number of threads for each of the 9 real graphs that we have. The results are shown below:

\begin{figure}
  \centering
  \includegraphics[scale=0.5]{figures/subgraphSize.png}
  \caption{ Subgraph sizes }
\end{figure}


\textbf{Automatic Pass determination}\
In the second set of experiments, we allow the program to determine the number of GPU passes it requires. We have seen that if the CPU has fewer than 200 conflicts to solve it is very fast(It is better to use a percentage, like percentage of the total number of vertices, but the stop criterion dose not have math ). (How did you arrive at this number of 200?)  - can we instead say that we move over to CPU when the number of remaining conflicts is negligible So after each pass we check to see the number of conflicts and if it is approximately less than 200, we terminate the GPU Passes and send the remaining conflicts to the CPU for solving.\

\begin{figure}
  \centering
  \includegraphics[scale=0.4]{figures/automotiveColors.png}
  \caption{ Subgraph sizes }
\end{figure}

Refer to the figures.  Need better labels (graph names rather than a,b,c).  SDO$\&$LDO is approximately twice as fast as compared to MAX or MIN. However, the number of colors for MAX and MIN is usually better than SDO$\&$LDO, with MAX nearly consistently yielding  fewer colors. For this set of graphs, the MAX heuristic also yields fewer colors that the sequential SDO$\&$LDO algorithm and the sequential First Fit   (FF: 42, 57, 57 - SDO: 42, 51, 51 - Best form other Paper: 44, 57,57 removed)  Also should talk about relationship between number of threads and colors, and number of threads and performance.  Where in these graphs does the performance associated with 8 vertices per subgraph fall?  Need to make a connection back to that.\

For this second set of tests, we see that though we easily beat the sequential First Fit in terms of color allocated, for ct20stif and nasasrb, we do not get consistently get fewer or match the colors of sequential SDO$\&$LDO while for the pwtk graph, we are well below the 48 colors obtained from running sequential SDO.  Do you have a sense why you are using fewer colors?

For ct20stif, in our 10 test runs we sometimes get 47 colors which beat the sequential SDO implementation but on average we get more colors and the best performances are obtained with parallel First Fit. Where is non-determinism coming from?  The conflict resolution?  You should probably elaborate on that.
(FF: 50, 47, 52 - SDO: 48, 36, 48 - Best from other Paper: 47,38,41)\

\subsection{Using METIS}
METIS is a set of algorithms that can be used to partition a graph into subgraphs. Many parallel graph algorithms advise the use of METIS to have better partitioning. However these algorithms run on parallel machines and they do not divide graphs in as many partitions as we do. Consequently when we tried to run METIS to partition the graph as we need, the resulting partitions were very unbalanced. The figure below shows the subgraph sizes from metis.


\subsection{Conclusion}

\subsection{Reference}



\appendix
\section{Appendix Title}

This is the text of the appendix, if you need one.

\acks

Acknowledgments, if needed.

% We recommend abbrvnat bibliography style.

\bibliographystyle{abbrvnat}
\bibliography{reference}

% The bibliography should be embedded for final submission.

%\begin{thebibliography}{}
%\softraggedright

%\bibitem[Smith et~al.(2009)Smith, Jones]{smith02}
%P. Q. Smith, and X. Y. Jones. ...reference text...




%\end{thebibliography}

\end{document}

